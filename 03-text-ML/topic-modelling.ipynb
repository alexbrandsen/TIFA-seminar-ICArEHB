{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841628d7-a496-4a45-893e-dcd3e7a7577d",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this notebook, you'll do some topic modelling on a selection of 144 papers about the use of fire in the paleolithic in Europe. We'll try to see if there are clusters of papers that have a similar topic, to see if there's any running themes in this research field. \n",
    "\n",
    "As you might expect from the word 'clusters', topic modelling is a unsupervised learning method, so we don't have any labels for the texts, just the texts themselves. A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"pyramid\" and \"pharaoh\" will appear more often in documents about Egyptian archaeology, \"Stonehenge\" and \"Beaker Culture\" will appear in documents about British archaeology, and \"the\" and \"is\" will appear approximately equally in both.\n",
    "\n",
    "We'll use the LDAvis package to do all the heavy lifting, we just need to preprocess the data and feed it into a function. But let's start with installing and importing some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45069b-dac5-438b-9768-4e42859cef7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install packages, if needed\n",
    "#!pip install git+https://github.com/bmabey/pyLDAvis.git\n",
    "!pip install pandas wordcloud scikit-learn seaborn pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838a95a-b9c8-487b-b545-153daf75c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "#from pyLDAvis import sklearn as sklearn_lda\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pickle \n",
    "from sklearn.preprocessing import normalize\n",
    "import operator\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set settings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c349a9-8689-4b07-9441-8e72d191940b",
   "metadata": {},
   "source": [
    "We'll also need some functions later on in this notebook, let's define those here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3e8a6-d86f-453a-8b72-ded228558b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some functions we'll use later\n",
    "\n",
    "# Create and display a wordcloud based on text\n",
    "def display_wordcloud(text):\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', width = 800, height = 600)\n",
    "    \n",
    "    # Generate a word cloud\n",
    "    wordcloud.generate(text)\n",
    "\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Visualize the word cloud\n",
    "    plt.imshow(wordcloud) \n",
    "\n",
    "# Display top 10 most common words\n",
    "def plot_most_common_words(count_data, count_vectorizer, number):\n",
    "    #import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names_out()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:number]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='Most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x=x_pos, y=counts, hue=x_pos, legend=None)\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "# show topics\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        number = topic_idx+1\n",
    "        print(\"\\nTopic #%d:\" % number)\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47395b-8f41-4cb5-ac91-f7774ee50dd0",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Ok, time to load the texts, they're in the data/fire-papers/ folder if you want to have a look at the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3031a-68f9-460b-9057-65f7154c691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location of txt files\n",
    "txt_path = 'data/fire-papers/'\n",
    "\n",
    "# load the content of each paper, save in a dataframe\n",
    "data = []\n",
    "txtfiles = os.listdir(txt_path)\n",
    "for txtfile in txtfiles:\n",
    "    filelocation = f\"{txt_path}{txtfile}\"\n",
    "    txt = open(filelocation, \"r\", encoding = 'utf-8').read().replace(\"Ô¨Å\", \"fi\") # replace joined 'fi' character with separate f and i\n",
    "    filename = txtfile.replace('.txt','')\n",
    "    data.append([filename,txt])\n",
    "\n",
    "papers = pd.DataFrame.from_records(data, columns=['filename', 'txt'])\n",
    "\n",
    "# check what that looks like, the head() function shows the top 5 rows\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e465d5-c505-4b7a-b304-34a40bd4b400",
   "metadata": {},
   "source": [
    "Before we do anything, it's worth making a wordcloud showing the most common words. That way we can get a quick overview of the contents of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b50c4-6856-41e3-a81a-0548c034533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different titles together, and display wordcloud\n",
    "all_text = ','.join(list(papers['txt'].values))\n",
    "display_wordcloud(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae96c90-b88d-4dea-8d3b-25afc85dfda6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Looking pretty good already, but let's do some basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2071e-bd2b-4bb8-a0d5-2bdd7c40ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['txt'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Remove numbers\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: re.sub('[0-9]+', '', x))\n",
    "\n",
    "# Convert to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the 5 preprocessed texts\n",
    "print(papers['paper_text_processed'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ca465-f38b-46a1-89dd-419dd52dc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different processed titles together, and display wordcloud again\n",
    "processed_text = ','.join(list(papers['paper_text_processed'].values))\n",
    "display_wordcloud(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2e1c2-8bf7-4b93-879b-30ffa6e5bbe0",
   "metadata": {},
   "source": [
    "Looking better, but there's still some words in here that are pretty useless when trying to find sub-themes of fire research, such as the words 'et al' are super common, and also 'fig' is not useful. Let's delete these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3118b77-c769-49b8-94c4-2ad34d52ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['paper_text_processed'] = papers['paper_text_processed'].str.replace('et al', '') # remove et al\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].str.replace('fig', '') # remove fig\n",
    "\n",
    "\n",
    "# Join the different processed titles together, and display wordcloud again\n",
    "processed_text = ','.join(list(papers['paper_text_processed'].values))\n",
    "display_wordcloud(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee551d-1e5c-41e2-b4cb-b082d67e5b85",
   "metadata": {},
   "source": [
    "Not bad! Do you see any other words that you think might not be useful for topic modelling? You can go back to the previous cell and add code to delete those words, if you wish.\n",
    "\n",
    "Once you're done, continue with the next cell to set up a so called 'vectoriser', this turns the words into vectors, which can then be used for machine learning, and this is the format that LDA expects. We also lemmatize the words, and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755a06f-732c-4540-bb7d-a684c7fba6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wordcloud automatically filters out punctuation, but LDA doesn't, so let's filter those out as well\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "# set up lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles) if len(t) > 2 ]\n",
    "\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    stop_words=stopwords.words('english'),\n",
    ")\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(papers['paper_text_processed'])\n",
    "\n",
    "# don't worry about the warning below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee39812-194f-4920-9ef1-7eec58d7f6b9",
   "metadata": {},
   "source": [
    "Now the words are counted, we can easily display the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857e225-754c-4122-aa1b-2eb862759d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the most common words\n",
    "plot_most_common_words(count_data, count_vectorizer,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e9a93-f861-4437-96f3-485f3ab51d00",
   "metadata": {},
   "source": [
    "Looking good, but there are some words in there that do not make a lot of sense, for example 'wa', 'also', and 'journal'. Let's remove stop words, and add some words of our own we want to delete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707d4b0-0035-435f-b6c5-828c0f88deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom stop words list, by starting with the default NLTK one, then adding our own terms\n",
    "custom_stopwords = stopwords.words('english')\n",
    "extra_stopwords = ['wa','also','journal']\n",
    "custom_stopwords += extra_stopwords\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    stop_words=custom_stopwords,\n",
    ")\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(papers['paper_text_processed'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_most_common_words(count_data, count_vectorizer, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c615bb8-3481-40b2-a44e-26500e8498e9",
   "metadata": {},
   "source": [
    "Do you see any other words that should be deleted? Go back to the previous cell and add more words to the extra_stopwords list. You can also change the number (20) when we call the function 'plot_most_common_words' if you want to see more words. Once you think the data is looking good, continue below.\n",
    "\n",
    "## Train\n",
    "\n",
    "Now we've done all the preprocessing, it's time to actually train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6818fa-3b49-4cd4-bfff-6b97b21d6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters (come back here later!)\n",
    "number_topics = 3\n",
    "number_words = 30\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede1cd8-da8b-43e6-ac7c-208779bc679e",
   "metadata": {},
   "source": [
    "Ok, we've got 3 clusters as found by LDA, now comes the hard part: trying to think of a topic name for each of the clusters. By looking at the top words for each topic, can you define or guess what the theme is? \n",
    "\n",
    "To make this a bit easier and intuitive, we can also create an interactive topic explorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e0cbb-b0ff-4f25-bd57-269388ab2d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export to interactive HTML file\n",
    "LDAvis_prepared = pyLDAvis.lda_model.prepare(lda, count_data, count_vectorizer)\n",
    "output_location = 'lda-output/ldavis_'+ str(number_topics) +'_topics.html'\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'lda-output/ldavis_'+ str(number_topics) +'_topics.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe5cbe-c6ae-4166-93ec-d21e2f066b28",
   "metadata": {},
   "source": [
    "The HTML file is saved under lda-output/ldavis_3_topics.html, which you can open with your browser, but we can also open it within jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1116b3-e4a9-4acb-a2e5-b8122c0fca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display interactive HTML\n",
    "HTML(filename=output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765ce94-1340-4472-b595-efa1413c7981",
   "metadata": {},
   "source": [
    "Now we don't just see the words per topic, but also how much each words contributes to a topic, and a cluster visualisation on the left. The size of the circles indicates how many documents are in that cluster, and the distance between the circles indicates how different they are. So currently, with 3 topics, they're all fairly different. \n",
    "\n",
    "Now comes the second hard part of topic modelling: figuring out how many topics is the optimal number. Go back to the cell where we set the number of topics, and update it from 3 to any other number. Play around and see if you can find a number where the topics are distinct, but not containing too many different words/themes. \n",
    "\n",
    "## Export the topics\n",
    "\n",
    "Optional: once you're happy with what the topics look like, we can export a CSV so we can see which topic has been assigned to each paper. Once you have run the cell below, you can find the file at lda-output/topics_per_paper.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137e1c2-65d9-463e-bd98-1f6a8641e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of topic per document\n",
    "txts = papers['paper_text_processed'].tolist()\n",
    "\n",
    "tf = count_vectorizer.fit_transform(txts)\n",
    "doc_topic = lda.transform(tf)\n",
    "\n",
    "topics = []\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    rounded = np.around(doc_topic[n], decimals=3)\n",
    "    ordered = {}\n",
    "    i = 1\n",
    "    for j in rounded:\n",
    "        ordered[i] = j\n",
    "        i += 1\n",
    "    ordered = sorted(ordered.items(), key=operator.itemgetter(1))\n",
    "    ordered.reverse()\n",
    "    \n",
    "    doctopics = ''\n",
    "    for k in ordered:\n",
    "        if k[1] > 0.20:\n",
    "            doctopics += '|{}|{}'.format(k[0],k[1])\n",
    "            \n",
    "    doctopics = doctopics[1:]\n",
    "    topics.append(doctopics)\n",
    " \n",
    "\n",
    "# add topic numbers to df, drop txt columns, save to csv\n",
    "papers['topic_numbers'] = topics\n",
    "del papers['paper_text_processed']\n",
    "del papers['txt']\n",
    "papers.to_csv('lda-output/topics_per_paper.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faebb58-2c93-414a-8a47-ec0972fdbefb",
   "metadata": {},
   "source": [
    "And that's all for this assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573d861-20f4-4cf1-a052-939f2663fac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
