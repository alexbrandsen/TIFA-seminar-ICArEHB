{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Sequence Labeling\n",
    "<ul><li>Tokenize</li>\n",
    "<ul><li>Functions</li>\n",
    "<li>For-Loops</li></ul>\n",
    "<li>Part of Speech Tags</li>\n",
    "<ul><li>Conditional Statements</li></ul>\n",
    "<li>Named Entity Recognition</li>\n",
    "<li>Geographic Bias?</li>\n",
    "<li>Archaeo-NER with Deep Learning</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some packages to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules (using pip)\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NLTK has access to appropriate models for this exercise, if not, they will be downloaded (this might take a little while)\n",
    "modules = [\"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"punkt\", \"words\"]\n",
    "\n",
    "for module in modules:\n",
    "    nltk.download(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the opening paragraph of Renfrew & Bahn's \"Archaeology: Theories, Methods and Practice\" throughout this notebook for our exercises\n",
    "# (Note the slashes at the end of each line, these allow you to put a longer string into a variable without having to put it all one one line)\n",
    "\n",
    "paragraph = 'Archaeology is partly the discovery of the treasures of the \\\n",
    "past, partly the work of the scientific analyst, partly the \\\n",
    "exercise of the creative imagination. It is toiling in the sun \\\n",
    "on an excavation in the deserts of Central Asia, it is working \\\n",
    "with living Inuit in the snows of Alaska. It is diving down to \\\n",
    "Spanish wrecks off the coast of Florida, and it is investigating  \\\n",
    "the sewers of Roman York. But it is also the analysis \\\n",
    "of materials in the laboratory, and the interpretation of \\\n",
    "what these things mean for the human story. Finally, \\\n",
    "archaeology is also the conservation of the world’s cultural \\\n",
    "heritage, the understanding of stakeholders of the past, \\\n",
    "and the protection of the past from looting and careless destruction' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is the field and set of methods dedicated to converting human language into something that the computer can read. It's important to keep in mind that a computer does not even know what a <i>word</i> is without receiving direct instructions from a human.\n",
    "\n",
    "Fortunately NLTK has an easy-to-implement set of instructions encoded in its function <i>word_tokenize()</i>. The idea with this function is that we can put a string of human-language text in between its parentheses and it will return a list of the individual words from that text. NLTK has a similar function, as well, called <i>sent_tokenize()</i> that does the same thing, but returns a list of individual sentences.\n",
    "\n",
    "Very often we want to tokenize our texts by word, while retaining infomation about the boundaries between sentences. In order to do this, we will first use <i>sent_tokenize()</i> and then iterate through our list of sentences with <i>word_tokenize()</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions we will use directly\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our sentence to a variable\n",
    "ylvis = \"What is the purpose of Stonehenge?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect our new variable\n",
    "ylvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed our new variable into the function\n",
    "word_tokenize(ylvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also assign the output of a function to a variable\n",
    "ylvis_list = word_tokenize(ylvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output variable\n",
    "ylvis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can input that new variable into other functions and so on. This 'len()' function counts the number of items in a list\n",
    "len(ylvis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign three sentences of dialogue to a new variable\n",
    "three_sentences = \"What is the purpose of Stonehenge? A giant granite birthday cake? Or a prison far too easy to escape?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the newest variable\n",
    "three_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text by word\n",
    "word_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text by sentence\n",
    "sent_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Use the function word_tokenize() in order to get a list of words\n",
    "##           from the paragraph from Renfrew & Bahn at the top of this notebook. \n",
    "##           How many tokens does the paragraph contain?\n",
    "\n",
    "## EXERCISE. Use the function sent_tokenize() in order to get a list of sentences\n",
    "##           from the Renfrew & Bahn paragraph.\n",
    "\n",
    "## Bonus: What is the average number of words per sentence in this paragraph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For-Loops\n",
    "\n",
    "We iterate through the elements in a list using the \"for\" and \"in\" syntax. You can tell those words do something special because they appear in green!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentence- and word-level tokenization\n",
    "\n",
    "# The line below gets indented, so that our script knows what to do\n",
    "# to each element in the list when it comes up\n",
    "\n",
    "sentence_list = sent_tokenize(three_sentences) # save sentences in variable\n",
    "\n",
    "for sentence in sentence_list: # tell Python to go through all sentences in the sentence_list and..\n",
    "    print(word_tokenize(sentence)) # ... print the list of tokens in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. For the paragraph from Renfrew & Bahn from earlier, use a for-loop to get\n",
    "##           a list of words from each sentence individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour: Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those not-yet-familiar with Natural Language Processing, it often comes as a surprise how powerful word frequencies are. Simply creating a list of the unique words in a text and tallying the number of times it appears encodes information about authorship, genre, time period and author nationality among other features. Frankly, this is mind boggling!\n",
    "\n",
    "It is exceptionally easy to create this kind of tally in Python. There is a simple out-of-the-box function that we can use to count the number of times a token appears in a list: <i>Counter</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a handy counting function\n",
    "# Reports number of time each unique element appears in a list\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokens\n",
    "indiana_jones_quote = \"If you want to be a good archeologist, you have got to get out of the library!\"\n",
    "jones_tokens = word_tokenize(indiana_jones_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect token list\n",
    "jones_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally the appearances of each unique token\n",
    "Counter(jones_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tally to a new variable\n",
    "tokens_counted = Counter(jones_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return unique tokens, sorted by number of appearances in list\n",
    "tokens_counted.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. What is the most common word in the Renfrew & Bahn paragraph from earlier?\n",
    "##           How often does 'past' appear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As trained readers, we know that language partly operates according to (or sometimes against!) abstract, underlying structures, such as grammar. Identifying a word's part of speech, or tagging it, is an extremely sophisticated task that remains an open problem in the Natural Language Processing world. At this point, state-of-the-art taggers have somewhere in the neighborhood of 98% accuracy.\n",
    "\n",
    "NLTK's default tagger, <i>pos_tag()</i>, has an accuracy just shy of that with the trade-off that it is comparatively fast. Simply place a list of tokens between its parentheses and it returns a new list where each item is the original word alongside its predicted part of speech.\n",
    "\n",
    "The tags themselves come from the Penn Treebank and a full list of them can be found here: <a href=\"http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common POS taggers\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.perceptron</td>\n",
    "        <td>import PerceptronTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.brill</td>\n",
    "        <td>import BrillTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.stanford</td>\n",
    "        <td>import StanfordTagger, StanfordPOSTagger, StanfordNERTagger</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: NLTK simply offers a wrapper for the Stanford taggers, which allows you to use them in Python, rather than their native Java. Stanford models can be downloaded from here: http://nlp.stanford.edu/software/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's current default POS tagger is the 'averaged perceptron' as described here:\n",
    "# https://spacy.io/blog/part-of-speech-POS-tagger-in-python\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Create variable for new sentence\n",
    "new_sentence = \"Broadly, archaeology is the study of the human past through its material remains.\"\n",
    "\n",
    "# Create list of word tokens\n",
    "new_tokens = word_tokenize(new_sentence)\n",
    "\n",
    "# Assign a POS tag to each token\n",
    "pos_tag(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not familiar with part of speech tags, anything starting with 'N' is a noun, and anything starting with a 'V' is a verb. Do the verbs and nouns make sense above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refresh ourselves on the functions and for-loops from earlier\n",
    "\n",
    "# An old variable revisited!\n",
    "three_sentences = \"What is the purpose of Stonehenge? A giant granite birthday cake? Or a prison far too easy to escape?\"\n",
    "\n",
    "# Re-make the list of sentences from the text\n",
    "sentence_list = sent_tokenize(three_sentences)\n",
    "\n",
    "# Re-tokenize each sentence by word\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the list of lists of tokens\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now iterate through the tokenized sentences and POS tag them\n",
    "for sentence in tokenized_sentences:\n",
    "    print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the tagged sentences in a list\n",
    "tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Get POS tags for the very new sentence below.\n",
    "very_new_sentence = \"We do not follow maps to buried treasure, and X never, ever marks the spot.\"\n",
    "\n",
    "## EXERCISE. Get POS tags for the paragraph from Renfrew and Bahn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entries in each tagged sentence consist of a token-tag pair.\n",
    "# Sometimes we just want one of those values.\n",
    "\n",
    "# When the entries in a list are paired like the (token,tag) format above,\n",
    "# we can label the elements seperately while we iterate through\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, we can access either value in the pair\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add a condition: IF the condition is TRUE,\n",
    "# then the script continues with the next indented line.\n",
    "# Otherwise, it gets skipped!\n",
    "\n",
    "# Calling the noun tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='NN': # if the tag is a noun\n",
    "            print(token) # print the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the adjective tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='JJ':\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The double equals sign is a test of equality NOT a variable assignment\n",
    "\n",
    "5 == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Return the nouns from the opening paragraph of Renfrew & Bahn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among parts of speech, names and proper nouns are of particular significance, since they are the more-or-less unique keywords that identify phenomena of social relevance (including people, places, time periods, artefacts, etc). After all, there is just one <i>Germany</i>, and in an excavation report, a word like <i>Neolithic</i> typically acts as a more-or-less stable referent over the course of the text. (Or perhaps we are interested in thinking about the degree of instability with which it is used!)\n",
    "\n",
    "The identification of these kinds of names is referred to as Named Entity Recognition, or NER. The challenge is twofold. First, it has to be determined whether a name spans multiple tokens. (These multi-token grammatical units are referred to as <i>chunks</i>; the process, <i>chunking</i>.) Second, we would ideally distinguish among categories of entity. Is <i>Neolithic</i> a geographic location? Just who is this <i>Germany</i> I hear so much about?\n",
    "\n",
    "To this end, the function ne_chunk() receives a list of tokens including their parts of speech and returns a nested list where named entities' tokens are chunked together, along with their category as predicted by the computer.\n",
    "\n",
    "Unfortunately, standard NER methods (such as this one), only find standard named entities, such as persons, locations and organisations, and not the entities we are insterested in as archaeologists (artefacts, species, etc). Standard named entities are used here as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a fresh sentence containing several proper names\n",
    "ner_sentence = 'King Arthur is the sovereign over Britain and lord of the Round Table.'\n",
    "ner_tokens = word_tokenize(ner_sentence)\n",
    "pos_tags = pos_tag(ner_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the POS tags\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the NER funtion\n",
    "from nltk import ne_chunk\n",
    "\n",
    "chunks = ne_chunk(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK is finicky here, so we need to use 'print' to inspect\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! It has identified Arthur as a person, Britain as a location (GPE), and Round Table as an organisation, pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll iterate through our list of chunks. Name Entities are grouped\n",
    "# together into 'nltk.tree.Tree'. (This is an under-the-hood data type.)\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select just ones with the 'GPE' (Geo-Political Entity) designation, and only print the entity\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "        if chunk.label()=='GPE':\n",
    "            print(\" \".join([token[0] for token in chunk.leaves()])) # this just loops over the tokens in the entity, and joins them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we have multiple conditions -- i.e. multiple 'if' statements --\n",
    "# we can put them together on a line using 'and'.\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree and chunk.label()=='GPE':\n",
    "            print(\" \".join([token[0] for token in chunk.leaves()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Retrieve the place names (excluding POS tags) from the sentence below.\n",
    "\n",
    "swallow_skeptic = \"Oh yeah, an African swallow, maybe, but not a European swallow.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4. Assignment: Geography in Renfrew & Bahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excerpt of a chapter of Renfrew & Bahn has been added in the data folder. This bit of text describes how to survey and excavate, using examples from around the world.\n",
    "\n",
    "Using the techniques from this lesson, fill in the missing lines of code below. Then we can count the number of times each place name appears in this text and return a list of the most common place names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample text of Renfrew & Bahn chapter from file\n",
    "# Creates variable 'rb_text' with whole text in single string\n",
    "rb_text = open('data/r_and_b_sample.txt',encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# do the following steps:\n",
    "\n",
    "# - tokenise text\n",
    "\n",
    "\n",
    "# - get POS tags\n",
    "\n",
    "\n",
    "# - get NER tags (make sure to assign the NER tags to a variable called 'rb_ner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the below code, which will take the entities and count them:\n",
    "\n",
    "# create new list to hold all places\n",
    "places_found = []\n",
    "\n",
    "# loop through NER tags and print only places\n",
    "for chunk in rb_ner:\n",
    "    if type(chunk)==nltk.tree.Tree and chunk.label()=='GPE':\n",
    "            place = \" \".join([token[0] for token in chunk.leaves()])\n",
    "            places_found.append(place)\n",
    "\n",
    "# count places\n",
    "counted_places = Counter(places_found)\n",
    "\n",
    "# sort places by count\n",
    "sorted_counted_places = counted_places.most_common()\n",
    "\n",
    "# output\n",
    "sorted_counted_places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- Does the list of the most common names make sense? \n",
    "- Are there things that don't? \n",
    "- Do you see any errors?\n",
    "- Do you think there is any geographical bias in the examples used by Renfrew & Bahn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Archaeo-NER with Deep Learning\n",
    "\n",
    "So far, we've only done 'standard' NER, i.e. finding person names, place names, etc. But what if we want to find archaeologically relevant entities, such as time periods, artefacts, and contexts? Luckily, a model for this has already been trained! But, unlike the NLTK NER model we've been using, this one is based on Deep Learning, specifically the BERT architecture. We will load the ArchaeoBERT-NER model from HuggingFace (a repository of ML models), test it on a sentence, and then apply it to the Renfrew and Bahn chapter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\tifa-seminar-practical\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load modules\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at alexbrandsen/ArchaeoBERT-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# download the model (this might take a while) and set up the predictor (called a pipeline)\n",
    "pipe = pipeline(\"token-classification\", model=\"alexbrandsen/ArchaeoBERT-NER\", aggregation_strategy=\"average\")\n",
    "\n",
    "# don't worry about the 'error' below, it will still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MAT',\n",
       "  'score': 0.72215235,\n",
       "  'word': 'iron',\n",
       "  'start': 17,\n",
       "  'end': 21},\n",
       " {'entity_group': 'ART',\n",
       "  'score': 0.7399969,\n",
       "  'word': 'Axe',\n",
       "  'start': 22,\n",
       "  'end': 25},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9945275,\n",
       "  'word': 'Middle Ages',\n",
       "  'start': 35,\n",
       "  'end': 46},\n",
       " {'entity_group': 'CON',\n",
       "  'score': 0.9451454,\n",
       "  'word': 'ditch',\n",
       "  'start': 52,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test the pipeline on a test sentence\n",
    "test_sentence = \"We have found an iron Axe from the Middle Ages in a ditch.\"\n",
    "entities = pipe(test_sentence) # predict entities\n",
    "entities # show entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the predictor has correctly identified the material, artefact, time period and context in that sentence. We also get a score for each entity, this is an indicator of how sure the model is about the prediction, with 1 being high probability, and 0 being the lowest probability.\n",
    "\n",
    "It is predicting in the BIO format, so it's using B- to indicate the start of an entity, and I- to indicate the continuation of an entity. For example, 'Middle' is labeled as B-PER (beginning of a period entity) and 'Ages' is labelled I-PER (continuation of the period entity). \n",
    "\n",
    "Now let's apply this to the Renfrew and Bahn text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Places:\n",
      "Artefacts:\n",
      "Time Periods:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('modern', 6),\n",
       " ('prehistoric', 6),\n",
       " ('Roman', 4),\n",
       " ('1960s', 3),\n",
       " ('2008', 3),\n",
       " ('medieval', 2),\n",
       " ('1970s', 2),\n",
       " ('1987', 2),\n",
       " ('1974', 1),\n",
       " ('1940', 1),\n",
       " ('1985', 1),\n",
       " ('Viking', 1),\n",
       " ('twelfth century cs', 1),\n",
       " ('Iron Age', 1),\n",
       " ('Chaco - period', 1),\n",
       " ('900 u 8o CE', 1),\n",
       " ('World War I', 1),\n",
       " ('1918', 1),\n",
       " ('Mesopotamia', 1),\n",
       " ('Middle East', 1),\n",
       " ('650 ce', 1),\n",
       " ('1980', 1),\n",
       " ('mid - 1980s', 1),\n",
       " ('sixteenth - century', 1),\n",
       " ('1890', 1),\n",
       " ('1981', 1),\n",
       " ('1950s', 1),\n",
       " ('sixth century', 1),\n",
       " ('Modern', 1),\n",
       " ('1954', 1),\n",
       " ('third millennium bce', 1),\n",
       " ('2011', 1),\n",
       " ('Anglo - Saxon', 1),\n",
       " ('Romano - British', 1),\n",
       " ('late', 1),\n",
       " ('1980s', 1),\n",
       " ('Paleolithic', 1),\n",
       " ('1940s', 1),\n",
       " ('Native American', 1),\n",
       " ('European Neolithic', 1),\n",
       " ('1853', 1),\n",
       " ('sixteenth century ce', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These BERT models expect non-tokenised sentences as input, so let only split the sentences, but don't do tokenisation\n",
    "sentences = sent_tokenize(rb_text)\n",
    "\n",
    "# create the empty lists for the entities to go in\n",
    "places_found = []\n",
    "artefacts_found = []\n",
    "periods_found = []\n",
    "\n",
    "# loop through sentences, predict entities for each\n",
    "for sentence in sentences:\n",
    "    # predict entities\n",
    "    entities = pipe(sentence) \n",
    "    #print(sentence)\n",
    "    #print(entities)\n",
    "    # loop through each entity in sentence, check which type it is, add to relevant list\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] == 'LOC':\n",
    "            places_found.append(entity['word'])\n",
    "        elif entity['entity_group'] == 'ART':\n",
    "            artefacts_found.append(entity['word'])\n",
    "        elif entity['entity_group'] == 'PER':\n",
    "            periods_found.append(entity['word'])\n",
    "\n",
    "\n",
    "# count entities\n",
    "counted_places = Counter(places_found)\n",
    "counted_artefacts = Counter(artefacts_found)\n",
    "counted_periods = Counter(periods_found)\n",
    "\n",
    "# sort entities by count\n",
    "sorted_counted_places = counted_places.most_common()\n",
    "sorted_counted_artefacts = counted_artefacts.most_common()\n",
    "sorted_counted_periods = counted_periods.most_common()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, did you notice how much longer this took to process than when we used the NLTK entity tagger? That's because BERT is a deep learning model, and uses way more computational power to do predictions, especially when running on a CPU instead of a GPU. Now let's look at what the model predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Places:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('England', 5),\n",
       " ('Mexico City', 4),\n",
       " ('Rome', 4),\n",
       " ('Mexico', 3),\n",
       " ('Britain', 3),\n",
       " ('China', 2),\n",
       " ('New Mexico', 2),\n",
       " ('Maya', 2),\n",
       " ('Saudi', 2),\n",
       " ('Washington', 2),\n",
       " ('Egypt', 2),\n",
       " ('Illinois', 2),\n",
       " ('Israel', 2),\n",
       " ('France', 1),\n",
       " ('Lascaux', 1),\n",
       " ('Great Temple', 1),\n",
       " ('Troy', 1),\n",
       " ('aux Meadows in Newfoundland', 1),\n",
       " ('eastern North America', 1),\n",
       " ('Wessex', 1),\n",
       " ('Virú', 1),\n",
       " ('Mohenjodaro', 1),\n",
       " ('Pakistan', 1),\n",
       " ('Danebury in Hampshire', 1),\n",
       " ('Blue J,', 1),\n",
       " ('Braidwood', 1),\n",
       " ('Scotla', 1),\n",
       " ('Stonehenge', 1),\n",
       " ('Caracol', 1),\n",
       " ('Belize', 1),\n",
       " ('Mediterranean Sea', 1),\n",
       " ('Arabia', 1),\n",
       " ('Jordan', 1),\n",
       " ('Ethiopia', 1),\n",
       " ('Rift Valley', 1),\n",
       " ('South Africa', 1),\n",
       " ('Pennsylvania', 1),\n",
       " ('Meadowcroft Rockshelter', 1),\n",
       " ('Forest', 1),\n",
       " ('North America', 1),\n",
       " ('Netherlands', 1),\n",
       " ('Oaxaca', 1),\n",
       " ('St. Catherine ’ s Island', 1),\n",
       " ('Georgia', 1),\n",
       " ('Italy', 1),\n",
       " ('St. Peter ’ s Basilica in', 1),\n",
       " ('Birmingham', 1),\n",
       " ('Wroxeter in', 1),\n",
       " ('Shropshire', 1),\n",
       " ('Staffordshire', 1),\n",
       " ('Cefn Graeanog,', 1),\n",
       " ('North Wales', 1),\n",
       " ('East', 1),\n",
       " ('North', 1),\n",
       " ('East Coast', 1),\n",
       " ('South Asia', 1),\n",
       " ('British', 1),\n",
       " ('Turkey', 1),\n",
       " ('Caesarea', 1),\n",
       " ('Port Royal', 1),\n",
       " ('Jamaica', 1),\n",
       " ('Texas', 1),\n",
       " ('Mary Rose', 1),\n",
       " ('Red Bay Wreck', 1),\n",
       " ('Canada', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Places:')\n",
    "sorted_counted_places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this list with the location list made by NLTK, do you see any major differences?\n",
    "\n",
    "Let's also look at the artefacts and time periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefacts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metal', 4),\n",
       " ('terracotta', 2),\n",
       " ('pottery', 2),\n",
       " ('storage jars', 2),\n",
       " ('darns', 1),\n",
       " ('Texture', 1),\n",
       " ('SLAR', 1),\n",
       " ('potsherds', 1),\n",
       " ('rods', 1),\n",
       " ('painted walls', 1),\n",
       " ('TV cameras', 1),\n",
       " ('sonar', 1),\n",
       " ('objects', 1),\n",
       " ('Metal', 1),\n",
       " ('gold and silver metalwork', 1),\n",
       " ('plaster', 1),\n",
       " ('ash', 1),\n",
       " ('coins', 1),\n",
       " ('dinner plate', 1),\n",
       " ('posts', 1),\n",
       " ('amphorae', 1),\n",
       " ('cannon', 1),\n",
       " ('glassmaking', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Artefacts:')\n",
    "sorted_counted_artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Periods:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('modern', 6),\n",
       " ('prehistoric', 6),\n",
       " ('Roman', 4),\n",
       " ('1960s', 3),\n",
       " ('2008', 3),\n",
       " ('medieval', 2),\n",
       " ('1970s', 2),\n",
       " ('1987', 2),\n",
       " ('1974', 1),\n",
       " ('1940', 1),\n",
       " ('1985', 1),\n",
       " ('Viking', 1),\n",
       " ('twelfth century cs', 1),\n",
       " ('Iron Age', 1),\n",
       " ('Chaco - period', 1),\n",
       " ('900 u 8o CE', 1),\n",
       " ('World War I', 1),\n",
       " ('1918', 1),\n",
       " ('Mesopotamia', 1),\n",
       " ('Middle East', 1),\n",
       " ('650 ce', 1),\n",
       " ('1980', 1),\n",
       " ('mid - 1980s', 1),\n",
       " ('sixteenth - century', 1),\n",
       " ('1890', 1),\n",
       " ('1981', 1),\n",
       " ('1950s', 1),\n",
       " ('sixth century', 1),\n",
       " ('Modern', 1),\n",
       " ('1954', 1),\n",
       " ('third millennium bce', 1),\n",
       " ('2011', 1),\n",
       " ('Anglo - Saxon', 1),\n",
       " ('Romano - British', 1),\n",
       " ('late', 1),\n",
       " ('1980s', 1),\n",
       " ('Paleolithic', 1),\n",
       " ('1940s', 1),\n",
       " ('Native American', 1),\n",
       " ('European Neolithic', 1),\n",
       " ('1853', 1),\n",
       " ('sixteenth century ce', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Time Periods:')\n",
    "sorted_counted_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even the deep learning model specifically trained on archaeological data is not perfect! But the results are still more useful for archaeologists than just getting the standard named entities (person names, organisation names, etc). Now we have extracted these entities, we can use this information in further analyses. For example, you could look at time periods mentioned in texts to find out which periods they describe, or extract all the artefacts, link them to existing vocabularies, and use them as linked open data. \n",
    "\n",
    "That's all we'll be looking at when it comes to sequence labelling, you can now open the next exercise in this folder, if there's time left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
